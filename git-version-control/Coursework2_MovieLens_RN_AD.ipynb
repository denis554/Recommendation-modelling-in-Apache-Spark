{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# INM432 Big Data Coursework 2016/207 Part 2: Spakr Pipelines and Evaluation of Scaling of Algorithms\n",
    "\n",
    "### Team Members: Ryan Nazareth and Aimore Dutra "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## a) Choice of Dataset and Task \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[summary: string, movieId: string, rating: string, timestamp: string, userId: string]\n",
      "1223\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "# the imports are used creating the data frame\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate() # create a SparkSession \n",
    "# this gets us an RDD. (could also be done with RDD.textFile in this case)\n",
    "lines = spark.read.text(\"hdfs://saltdean/data/movielens/sample_movielens_ratings.txt\").rdd \n",
    "# now split the lines at the '::'\n",
    "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "ratings.createOrReplaceTempView('ratings') # register the DataFrame so that we can use it with Spark SQL.\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2]) # split into test and training set\n",
    "print(training.describe()) # just for testing, should show the four columns\n",
    "print(training.count()) # just fore testing, should be around 1188"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## b) Machine Learning Pipeline \n",
    "\n",
    "Now create an ALS estimator and a parameter grid to explore different values for the `rank` and `regParam` parameter of the ALS. \n",
    "\n",
    "### Choice of processing steps:\n",
    "\n",
    "### Learning algorithms: \n",
    "\n",
    "### Parameter settings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanRating 1.7741505662891406\n",
      "se_df DataFrame[se: double]\n",
      "meanSE 1.3087042746844497\n"
     ]
    }
   ],
   "source": [
    "SQL1 = 'SELECT AVG(rating) FROM ratings'\n",
    "row = spark.sql(SQL1).collect()[0] # get the single row with the result\n",
    "\n",
    "meanRating = row['avg(rating)'] # access Row as a map \n",
    "print('meanRating',meanRating)\n",
    "\n",
    "se_rdd = test.rdd.map(lambda row: Row(se = pow(row['rating']-meanRating,2)) ) \n",
    "se_df = spark.createDataFrame(se_rdd) \n",
    "se_df.createOrReplaceTempView('se')\n",
    "print('se_df',se_df)\n",
    "SQL2 = 'SELECT AVG(se) FROM se'\n",
    "row = spark.sql(SQL2).collect()[0]\n",
    "meanSE = row['avg(se)'] # access Row as a map \n",
    "print('meanSE',meanSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## c) Evaluating Performance of Pipeline using training and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting cross-validation\n",
      "finished cross-validation\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, rank=5, regParam=0.1, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
    "\n",
    "regEval = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "crossVal = CrossValidator(estimator=als, estimatorParamMaps=paramGrid, evaluator=regEval)\n",
    "print('starting cross-validation')\n",
    "cvModel = crossVal.fit(training)\n",
    "print('finished cross-validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## d) Implementing a parameter grid \n",
    "\n",
    "Implementing a parameter grid (using pyspark.ml.tuning.ParamGridBuilder[source]), varying at least one feature preprocessing step, one machine learning parameter, and the training set size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "({Param(parent='ALS_4c32810e0999be3026e9', name='rank', doc='rank of the factorization'): 10, Param(parent='ALS_4c32810e0999be3026e9', name='regParam', doc='regularization parameter (>= 0).'): 0.03}, 5.241072647683341)\n",
      "Root-mean-square error = 1.0655693202120597\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(als.regParam, [0.03,0.1,0.3]) \\\n",
    "    .addGrid(als.rank, [5,10,50]).build()\n",
    "    #    .addGrid(als.rank, [3,10,30,100,300]).build() \n",
    "\n",
    "print(cvModel.bestModel.rank)\n",
    "paramMap = list(zip(cvModel.getEstimatorParamMaps(),cvModel.avgMetrics))\n",
    "paramMax = max(paramMap, key=lambda x: x[1])\n",
    "print(paramMax)\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = cvModel.transform(test)\n",
    "rmse = regEval.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
