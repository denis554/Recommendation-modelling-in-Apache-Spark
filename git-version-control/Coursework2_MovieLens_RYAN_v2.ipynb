{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# INM432 Big Data Coursework 2016/207 Part 2: Spark Pipelines and Evaluation of Scaling of Algorithms\n",
    "\n",
    "### Team Members: Ryan Nazareth and Aimore Dutra \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Introduction\n",
    "With the advent of the internet connecting everything and everyone, it became easy to one access a large amount of information. However, the facility to reach so much data also brought some problems. Consumers have to deal with an immeasurable number of items, loosing their time trying to find what they look for.\n",
    "\n",
    "Hence, big companies that have an immensity of products in their database are keen on advertising their products in a smart way helping their clients to find what they want.\n",
    "\n",
    "Nowadays, Recommendation Systems are being developed to address this problem.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task\n",
    "\n",
    "Our task is to create a Recommender System that can suggest new movies to users based on their preferences (ratings).\n",
    "\n",
    "There are several possible approaches for the recommendation task [1]:\n",
    "\n",
    "##### 1) Recommend the most popular items\n",
    "\n",
    "##### 2) Use a classifier to make recommendation\n",
    "\n",
    "##### 3) Collaborative Filtering\n",
    "\n",
    "#### We chose the Collaborative Filtering technique because this method gives more personalization and makes a more efficient use of data.\n",
    "\n",
    "The Collaborative Filtering approach has two main types:\n",
    "* a) User to User\n",
    "* b) Item to Item\n",
    "\n",
    "Item-item most of the time tends to be more accurate and computationally cheaper.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> #### References\n",
    "[1] https://www.analyticsvidhya.com/blog/2016/06/quick-guide-build-recommendation-engine-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Dataset\n",
    "\n",
    "*Movies and most recently series have become a trend due to their current amazing quality and quantity at hand. Thanks to the advances in technology allowing them to be cheaper and quickly produced, there are millions of movies and series available.\n",
    "Not only more content is being created, but the existing ones are being stored. This has resulted in viewers having difficulties to find new video entertainment instances that they like.*\n",
    "\n",
    "\n",
    "\n",
    "The selected dataset for the coursework was the \"(ml-20m)\" from MovieLens, a movie recommendation service [1,2]. It contains: \n",
    "\n",
    "- 27,278 movies (with 19 different Genres)\n",
    "- 138,493 users\n",
    "- 465,564 tag applications \n",
    "- and 20,000,263 ratings (from 1-5 stars)\n",
    "\n",
    "These data were created by  users between January 09, 1995 and March 31, 2015.\n",
    "\n",
    "The data are divided in six files, containing each:\n",
    "- genome-scores.csv: movieId, tagId, relevance\n",
    "- genome-tags.csv: tagId, tag\n",
    "- links.csv: movieId, imdbId, tmdbId\n",
    "- movies.csv: MovieID::Title::Genres\n",
    "- ratings.csv: UserID::MovieID::Rating::Timestamp\n",
    "- tags.csv: userId, movieId, tag, timestamp\n",
    "\n",
    "\n",
    "> #### References\n",
    "[1] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872\n",
    "\n",
    ">[2] http://files.grouplens.org/datasets/movielens/ml-20m-README.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Technique Used\n",
    "Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix. spark.ml currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. spark.ml uses the alternating least squares (ALS) algorithm to learn these latent factors. The implementation in spark.ml has the following parameters:\n",
    "\n",
    "- numBlocks is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\n",
    "- rank is the number of latent factors in the model (defaults to 10).\n",
    "- maxIter is the maximum number of iterations to run (defaults to 10).\n",
    "- regParam specifies the regularization parameter in ALS (defaults to 1.0).\n",
    "- implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false which means using explicit feedback).\n",
    "- alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0).\n",
    "- nonnegative specifies whether or not to use nonnegative constraints for least squares (defaults to false).\n",
    "\n",
    "> #### References\n",
    "[1] https://spark.apache.org/docs/latest/ml-collaborative-filtering.html\n",
    "\n",
    ">[2] https://spark.apache.org/docs/latest/ml-tuning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Loading data and applying transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "import math \n",
    "\n",
    "spark = SparkSession.builder.getOrCreate() # create a SparkSession \n",
    "\n",
    "ratings = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://saltdean/data/movielens/ml-latest-small/ratings.csv\")\n",
    "\n",
    "ratings.describe() # lets check which features are present \n",
    "\n",
    "ratings = ratings.withColumn(\"movieId\", ratings[\"movieId\"].cast(IntegerType()))\n",
    "ratings = ratings.withColumn(\"rating\", ratings[\"rating\"].cast(DoubleType()))\n",
    "ratings = ratings.withColumn(\"timestamp\", ratings[\"timestamp\"].cast(IntegerType()))\n",
    "ratings = ratings.withColumn(\"userId\", ratings[\"userId\"].cast(IntegerType()))\n",
    "# ratings.take(3)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train and test set, setting parameter grid and estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "\n",
    "training.take(3)\n",
    "# THE ORDER I AM GETTING  Row(userId=1, movieId=31, rating=2.5, timestamp=1260759144)\n",
    "# THE CORRECT ORDER       Row(movieId=0, rating=1.0, timestamp=1424380312, userId=3)\n",
    "\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
    "\n",
    "# # We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "paramGrid = ParamGridBuilder().addGrid(als.regParam, [0.03,0.1,0.3]).addGrid(als.rank, [5,10,50]).build()\n",
    "    \n",
    "# # In this case the estimator is simply the linear regression.\n",
    "# # A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "regEval = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "tvs = TrainValidationSplit(estimator=als,\n",
    "                            estimatorParamMaps=paramGrid,\n",
    "                            evaluator=regEval,\n",
    "                            # 80% of the data will be used for training, 20% for validation.\n",
    "                            trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model, generating predictions and evaluating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was trained with rank 5\n",
      "\n",
      "\n",
      "Row(userId=232, movieId=463, rating=4.0, timestamp=955089443, prediction=3.7277157306671143)\n",
      "\n",
      "Row(userId=350, movieId=471, rating=3.0, timestamp=1011714986, prediction=3.852583646774292)\n",
      "\n",
      "Row(userId=306, movieId=471, rating=3.0, timestamp=939718996, prediction=3.836204767227173)\n",
      "\n",
      "Row(userId=92, movieId=471, rating=4.0, timestamp=848526594, prediction=3.782599449157715)\n",
      "\n",
      "Row(userId=299, movieId=471, rating=4.5, timestamp=1344186741, prediction=3.888803243637085)\n",
      "\n",
      "Root-mean-square error = 0.9801629526313057\n"
     ]
    }
   ],
   "source": [
    "# # Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(training)\n",
    "\n",
    "print ('The best model was trained with rank %d'%model.bestModel.rank)\n",
    "\n",
    "print('')\n",
    "\n",
    "## Computing the predictions\n",
    "                \n",
    "predictions = model.transform(test)\n",
    "\n",
    "# dropping any rows with nan values from prediction (due to cold start problem)               \n",
    "                \n",
    "predictions = predictions.dropna()  \n",
    "\n",
    "                \n",
    "# printing out the first five observations and predictions                 \n",
    "for row in predictions.take(5):\n",
    "    print('')\n",
    "    print(row)\n",
    "\n",
    "               \n",
    "# # # # # Evaluate the model by computing the RMSE on the test data\n",
    "                \n",
    "rmse = regEval.evaluate(predictions)\n",
    "print('')\n",
    "print(\"Root-mean-square error = \" + str(rmse))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "https://sourceforge.net/p/jupiter/wiki/markdown_syntax/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing training set size and applying all the steps above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was trained with rank 5\n",
      "\n",
      "\n",
      "Row(userId=452, movieId=463, rating=2.0, timestamp=976424451, prediction=3.046884059906006)\n",
      "\n",
      "Row(userId=534, movieId=463, rating=4.0, timestamp=973377486, prediction=3.7673137187957764)\n",
      "\n",
      "Row(userId=85, movieId=471, rating=3.0, timestamp=837512312, prediction=2.572993040084839)\n",
      "\n",
      "Row(userId=350, movieId=471, rating=3.0, timestamp=1011714986, prediction=4.318910598754883)\n",
      "\n",
      "Row(userId=602, movieId=471, rating=3.0, timestamp=842357922, prediction=3.957103729248047)\n",
      "\n",
      "Root-mean-square error = 1.0040720904641864\n"
     ]
    }
   ],
   "source": [
    "(training, test) = ratings.randomSplit([0.7, 0.3]) #Using smaller training set \n",
    "training.take(3)\n",
    "# THE ORDER I AM GETTING  Row(userId=1, movieId=31, rating=2.5, timestamp=1260759144)\n",
    "# THE CORRECT ORDER       Row(movieId=0, rating=1.0, timestamp=1424380312, userId=3)\n",
    "\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
    "\n",
    "# # We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "paramGrid = ParamGridBuilder().addGrid(als.regParam, [0.03,0.1,0.3]).addGrid(als.rank, [5,10,50]).build()\n",
    "    \n",
    "# # In this case the estimator is simply the linear regression.\n",
    "# # A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "regEval = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "tvs = TrainValidationSplit(estimator=als,\n",
    "                            estimatorParamMaps=paramGrid,\n",
    "                            evaluator=regEval,\n",
    "                            # 80% of the data will be used for training, 20% for validation.\n",
    "                            trainRatio=0.8)\n",
    "\n",
    "# # Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(training)\n",
    "\n",
    "print ('The best model was trained with rank %d'%model.bestModel.rank)\n",
    "\n",
    "print('')\n",
    "\n",
    "## Computing the predictions\n",
    "                \n",
    "predictions = model.transform(test)\n",
    "\n",
    "# dropping any rows with nan values from prediction (due to cold start problem)               \n",
    "                \n",
    "predictions = predictions.dropna()  \n",
    "\n",
    "                \n",
    "# printing out the first five observations and predictions                 \n",
    "for row in predictions.take(5):\n",
    "    print('')\n",
    "    print(row)\n",
    "\n",
    "               \n",
    "# # # # # Evaluate the model by computing the RMSE on the test data\n",
    "                \n",
    "rmse = regEval.evaluate(predictions)\n",
    "print('')\n",
    "print(\"Root-mean-square error = \" + str(rmse))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the training set size increases the root-mean square error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
