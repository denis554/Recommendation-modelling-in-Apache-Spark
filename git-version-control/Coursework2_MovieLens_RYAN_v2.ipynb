{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# INM432 Big Data Coursework 2016/207 Part 2: Spark Pipelines and Evaluation of Scaling of Algorithms\n",
    "\n",
    "### Team Members: Ryan Nazareth and Aimore Dutra \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1) Introduction\n",
    "With the advent of the internet connecting everything and everyone, it became easy to one access a large amount of information. However, the facility to reach so much data also brought some problems. Consumers have to deal with an immeasurable number of items, loosing their time trying to find what they look for.\n",
    "\n",
    "Hence, big companies that have an immensity of products in their database are keen on advertising their products in a smart way helping their clients to find what they want.\n",
    "\n",
    "Nowadays, Recommendation Systems are being developed to address this problem.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.1) Task\n",
    "\n",
    "Our task is to create a Recommender System that can suggest new movies to users based on their preferences (ratings).\n",
    "\n",
    "There are several possible approaches for the recommendation task [1]:\n",
    "\n",
    "##### 1) Recommend the most popular items\n",
    "##### 2) Use a classifier to make recommendation\n",
    "##### 3) Collaborative Filtering\n",
    "\n",
    "#### We chose the Collaborative Filtering technique because this method gives more personalization and makes a more efficient use of data.\n",
    "\n",
    "The Collaborative Filtering approach has two main types:\n",
    "* a) User to User\n",
    "* b) Item to Item\n",
    "\n",
    "Item-item most of the time tends to be more accurate and computationally cheaper.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> #### References\n",
    "[1] https://www.analyticsvidhya.com/blog/2016/06/quick-guide-build-recommendation-engine-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.2) Dataset\n",
    "\n",
    "*Movies and most recently series have become a trend due to their current amazing quality and quantity at hand. Thanks to the advances in technology allowing them to be cheaper and quickly produced, there are millions of movies and series available.\n",
    "Not only more content is being created, but the existing ones are being stored. This has resulted in viewers having difficulties to find new video entertainment instances that they like.*\n",
    "\n",
    "\n",
    "\n",
    "The selected dataset for the coursework was the \"(ml-20m)\" from MovieLens, a movie recommendation service [1,2]. We made this choice because it has a lot of data and it most important it contains user ratings that allow us to use the Collaborative Filtering technique. The details of the dataset is below: \n",
    "\n",
    "- 27,278 movies (with 19 different Genres)\n",
    "- 138,493 users\n",
    "- 465,564 tag applications \n",
    "- and 20,000,263 ratings (from 1-5 stars)\n",
    "\n",
    "These data were created by  users between January 09, 1995 and March 31, 2015.\n",
    "\n",
    "The data are divided in six files, containing each:\n",
    "- genome-scores.csv: MovieID::TagId::relevance\n",
    "- genome-tags.csv:   TagId::Tag\n",
    "- links.csv:         MovieID::imdbID::tmdbID\n",
    "- movies.csv:        MovieID::Title::Genres\n",
    "- ratings.csv:       UserID::MovieID::Rating::Timestamp\n",
    "- tags.csv:          UserID::MovieID::Tag::Timestamp\n",
    "\n",
    "\n",
    "> #### References\n",
    "[1] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872\n",
    "\n",
    ">[2] http://files.grouplens.org/datasets/movielens/ml-20m-README.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.3) Technique Used\n",
    "Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix. spark.ml currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. spark.ml uses the alternating least squares (ALS) algorithm to learn these latent factors. The implementation in spark.ml has the following parameters:\n",
    "\n",
    "- numBlocks is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\n",
    "- rank is the number of latent factors in the model (defaults to 10).\n",
    "- maxIter is the maximum number of iterations to run (defaults to 10).\n",
    "- regParam specifies the regularization parameter in ALS (defaults to 1.0).\n",
    "- implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false which means using explicit feedback).\n",
    "- alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0).\n",
    "- nonnegative specifies whether or not to use nonnegative constraints for least squares (defaults to false).\n",
    "\n",
    "> #### References\n",
    "[1] https://spark.apache.org/docs/latest/ml-collaborative-filtering.html\n",
    "\n",
    ">[2] https://spark.apache.org/docs/latest/ml-tuning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "# 2) Code\n",
    "\n",
    "\n",
    "### Loading data and applying transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "import math \n",
    "import time\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load data from the path to a dataframe called \"ratings\"\n",
    "## Small Dataset\n",
    "ratings = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://saltdean/data/movielens/ml-latest-small/ratings.csv\")\n",
    "## Large Dataset\n",
    "# ratings = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://saltdean/data/movielens/ml-20m/ratings.csv\")\n",
    "# lets check which features are present \n",
    "ratings.describe() \n",
    "\n",
    "# Cast data type from String to Integer and Double\n",
    "ratings = ratings.withColumn(\"movieId\", ratings[\"movieId\"].cast(IntegerType()))\n",
    "ratings = ratings.withColumn(\"rating\", ratings[\"rating\"].cast(DoubleType()))\n",
    "ratings = ratings.withColumn(\"timestamp\", ratings[\"timestamp\"].cast(IntegerType()))\n",
    "ratings = ratings.withColumn(\"userId\", ratings[\"userId\"].cast(IntegerType()))\n",
    "# ratings.take(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.1) Approach 1:\n",
    "- Split Training (80%) and Testing(20%) data\n",
    "- Do a Grid Search to select the best model\n",
    "- Predict test data using the best model\n",
    "- Evaluate the best model's performance and time taken for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Data size:  107  rows\n",
      "2\n",
      "3\n",
      "4\n",
      "The best model was trained with rank 10\n",
      "\n",
      "\n",
      "Training Error (RMS) = 0.0032372987037692395\n",
      "Training data size:  92  rows\n",
      "Training time:  12.917011260986328  seconds\n",
      "\n",
      "Row(userId=615, movieId=7438, rating=4.0, timestamp=1408778718, prediction=-0.30026188492774963)\n",
      "\n",
      "Test Error (RMS) = 4.30026188492775\n",
      "Test data size:  15  rows\n",
      "Test time:  0.07261943817138672  seconds\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training (80%) and hold-out testing data (20%)\n",
    "(data, nothing) = ratings.randomSplit([0.001, 0.999])\n",
    "print('1')\n",
    "(training, test) = data.randomSplit([0.8, 0.2])\n",
    "print('Data size: ',data.count(),' rows') \n",
    "# print('Test data size: ',test.count(),' rows') \n",
    "\n",
    "\n",
    "# Split the data into training (80%) and hold-out testing data (20%)\n",
    "# (training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "print('2')\n",
    "# Create Alternate Least Square\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
    "\n",
    "# Create a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# paramGrid = ParamGridBuilder().addGrid(als.regParam, [0.03,0.1,0.3]).addGrid(als.rank, [5,10,50]).addGrid(als.maxIter, [1,5,10]).build()\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "print('3')\n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "regEval = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "#### Train ####\n",
    "# Get start time\n",
    "s=time.time()\n",
    "print('4')\n",
    "# Train and Validate models\n",
    "tvs = TrainValidationSplit(estimator=als,\n",
    "                            estimatorParamMaps=paramGrid,\n",
    "                            evaluator=regEval,\n",
    "                            # 80% of the data will be used for training, 20% for validation.\n",
    "                            trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit to choose the best set of parameters.\n",
    "model = tvs.fit(training)\n",
    "print ('The best model was trained with rank %d'%model.bestModel.rank)\n",
    "print('')\n",
    "\n",
    "# Get end time\n",
    "e=time.time()\n",
    "\n",
    "# Test the model's prediction in the hold-out Training data           \n",
    "predictions = model.transform(training)\n",
    "\n",
    "# Drop any rows with nan values from prediction (due to cold start problem)\n",
    "predictions = predictions.dropna()\n",
    "\n",
    "# Evaluate the overall performance of the model by computing the Root-mean-square error (RMSE) on the Training data\n",
    "rmse = regEval.evaluate(predictions)\n",
    "print('')\n",
    "print(\"Training Error (RMS) = \" + str(rmse))\n",
    "\n",
    "# Print the size of training data\n",
    "print('Training data size: ',training.count(),' rows')  \n",
    "     \n",
    "# Print the time spent to train\n",
    "print('Training time: ',e-s,' seconds')\n",
    "\n",
    "#### Test ####\n",
    "# Get relative time\n",
    "s=time.time()\n",
    "\n",
    "# Test the model's prediction in the hold-out Test data           \n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Get end time\n",
    "e=time.time()\n",
    "\n",
    "# Drop any rows with nan values from prediction (due to cold start problem)\n",
    "predictions = predictions.dropna()\n",
    "                \n",
    "# Print the first 5 observations and predictions\n",
    "for row in predictions.take(5):\n",
    "    print('')\n",
    "    print(row)\n",
    "               \n",
    "# Evaluate the overall performance of the model by computing the Root-mean-square error (RMSE) on the Test data\n",
    "rmse = regEval.evaluate(predictions)\n",
    "print('')\n",
    "print(\"Test Error (RMS) = \" + str(rmse))\n",
    "\n",
    "# Print the size of test data\n",
    "print('Test data size: ',test.count(),' rows')  \n",
    "     \n",
    "# Print the time spent to test\n",
    "print('Test time: ',e-s,' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.2) Approach 2:\n",
    "\n",
    "### Reducing training set size and applying all the steps above\n",
    "\n",
    "- Split Training (60%) and Testing(40%) data\n",
    "- Do a Grid Search to select the best model\n",
    "- Predict test data using the best model\n",
    "- Evaluate the best model's performance and time taken for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split the data into training (60%) and hold-out testing data (40%)\n",
    "(training, test) = ratings.randomSplit([0.6, 0.4])\n",
    "\n",
    "\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
    "\n",
    "# Create a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "paramGrid = ParamGridBuilder().addGrid(als.regParam, [0.03,0.1,0.3]).addGrid(als.rank, [5,10,50]).build()\n",
    "    \n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "regEval = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "## Train ##\n",
    "# Get relative time\n",
    "s=time.time()\n",
    "\n",
    "# Train and Validate models\n",
    "tvs = TrainValidationSplit(estimator=als,\n",
    "                            estimatorParamMaps=paramGrid,\n",
    "                            evaluator=regEval,\n",
    "                            # 80% of the data will be used for training, 20% for validation.\n",
    "                            trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit to choose the best set of parameters.\n",
    "model = tvs.fit(training)\n",
    "print ('The best model was trained with rank %d'%model.bestModel.rank)\n",
    "print('')\n",
    "\n",
    "# Print the time spent to train\n",
    "print('Training time:',time.time()-s,' seconds')\n",
    "\n",
    "## Test ##\n",
    "# Get relative time\n",
    "s=time.time()\n",
    "\n",
    "# Test the model's prediction in the hold-out Test data           \n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Print the time spent to test\n",
    "print('Test time:',time.time()-s,' seconds')\n",
    "\n",
    "# Drop any rows with nan values from prediction (due to cold start problem)\n",
    "predictions = predictions.dropna()\n",
    "                \n",
    "# Print the first 5 observations and predictions\n",
    "for row in predictions.take(5):\n",
    "    print('')\n",
    "    print(row)\n",
    "               \n",
    "## Evaluate the overall performance of the model by computing the RMSE on the test data\n",
    "rmse = regEval.evaluate(predictions)\n",
    "print('')\n",
    "print(\"Root-mean-square error = \" + str(rmse))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3) Conclusions and Discussions\n",
    "Reducing the training set size increases the root-mean square error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "https://sourceforge.net/p/jupiter/wiki/markdown_syntax/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
