{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# INM432 Big Data Coursework 2016/207 Part 2: Spark Pipelines and Evaluation of Scaling of Algorithms\n",
    "\n",
    "### Team Members: Ryan Nazareth and Aimore Dutra \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1) Introduction\n",
    "With the advent of the internet connecting everything and everyone, it became easy to one access a large amount of information. However, the facility to reach so much data also brought some problems. Consumers have to deal with an immeasurable number of items, loosing their time trying to find what they look for.\n",
    "\n",
    "Hence, big companies that have an immensity of products in their database are keen on advertising their products in a smart way helping their clients to find what they want.\n",
    "\n",
    "Nowadays, Recommendation Systems are being developed to address this problem.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.1) Task\n",
    "\n",
    "Our task is to create a Recommender System that can suggest new movies to users based on their preferences (ratings).\n",
    "\n",
    "There are several possible approaches for the recommendation task [1]:\n",
    "\n",
    "##### 1) Recommend the most popular items\n",
    "##### 2) Use a classifier to make recommendation\n",
    "##### 3) Collaborative Filtering\n",
    "\n",
    "#### We chose the Collaborative Filtering technique because this method gives more personalization and makes a more efficient use of data.\n",
    "\n",
    "The Collaborative Filtering approach has two main types:\n",
    "* a) User to User\n",
    "* b) Item to Item\n",
    "\n",
    "Item-item most of the time tends to be more accurate and computationally cheaper.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> #### References\n",
    "[1] https://www.analyticsvidhya.com/blog/2016/06/quick-guide-build-recommendation-engine-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.2) Dataset\n",
    "\n",
    "*Movies and most recently series have become a trend due to their current amazing quality and quantity at hand. Thanks to the advances in technology allowing them to be cheaper and quickly produced, there are millions of movies and series available.\n",
    "Not only more content is being created, but the existing ones are being stored. This has resulted in viewers having difficulties to find new video entertainment instances that they like.*\n",
    "\n",
    "\n",
    "\n",
    "The selected dataset for the coursework was the \"(ml-20m)\" from MovieLens, a movie recommendation service [1,2]. We made this choice because it has a lot of data and it most important it contains user ratings that allow us to use the Collaborative Filtering technique. The details of the dataset is below: \n",
    "\n",
    "- 27,278 movies (with 19 different Genres)\n",
    "- 138,493 users\n",
    "- 465,564 tag applications \n",
    "- and 20,000,263 ratings (from 1-5 stars)\n",
    "\n",
    "These data were created by  users between January 09, 1995 and March 31, 2015.\n",
    "\n",
    "The data are divided in six files, containing each:\n",
    "- genome-scores.csv: MovieID::TagId::relevance\n",
    "- genome-tags.csv:   TagId::Tag\n",
    "- links.csv:         MovieID::imdbID::tmdbID\n",
    "- movies.csv:        MovieID::Title::Genres\n",
    "- ratings.csv:       UserID::MovieID::Rating::Timestamp\n",
    "- tags.csv:          UserID::MovieID::Tag::Timestamp\n",
    "\n",
    "\n",
    "> #### References\n",
    "[1] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872\n",
    "\n",
    ">[2] http://files.grouplens.org/datasets/movielens/ml-20m-README.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.3) Learning Algorithm - ALS\n",
    "\n",
    "Collaborative filtering is commonly used for recommender systems [1]. These techniques aim to fill in the missing entries of a user-item association matrix. spark.ml currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. spark.ml uses the alternating least squares (ALS) algorithm to learn these latent factors. The implementation in spark.ml has the following parameters:\n",
    "\n",
    "- **numBlocks** is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\n",
    "- **rank** is the number of latent factors in the model (defaults to 10).\n",
    "- **maxIter** is the maximum number of iterations to run (defaults to 10).\n",
    "- **regParam** specifies the regularization parameter in ALS (defaults to 1.0).\n",
    "- **implicitPrefs** specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false which means using explicit feedback).\n",
    "- **alpha** is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0).\n",
    "- **nonnegative** specifies whether or not to use nonnegative constraints for least squares (defaults to false).\n",
    "\n",
    "\n",
    "### Train-Validation Split\n",
    "In addition to CrossValidator Spark also offers TrainValidationSplit for hyper-parameter tuning. TrainValidationSplit only evaluates each combination of parameters once, as opposed to k times in the case of CrossValidator. It is therefore less expensive, but will not produce as reliable results when the training dataset is not sufficiently large. [2]\n",
    "\n",
    "> #### References\n",
    "[1] https://spark.apache.org/docs/latest/ml-collaborative-filtering.html\n",
    "\n",
    ">[2] https://spark.apache.org/docs/latest/ml-tuning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "# 2) Code\n",
    "\n",
    "\n",
    "## 2.1) Loading data and applying transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings\n",
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|     31|   2.5|1260759144|\n",
      "|     1|   1029|   3.0|1260759179|\n",
      "|     1|   1061|   3.0|1260759182|\n",
      "|     1|   1129|   2.0|1260759185|\n",
      "|     1|   1172|   4.0|1260759205|\n",
      "|     1|   1263|   2.0|1260759151|\n",
      "|     1|   1287|   2.0|1260759187|\n",
      "|     1|   1293|   2.0|1260759148|\n",
      "|     1|   1339|   3.5|1260759125|\n",
      "|     1|   1343|   2.0|1260759131|\n",
      "|     1|   1371|   2.5|1260759135|\n",
      "|     1|   1405|   1.0|1260759203|\n",
      "|     1|   1953|   4.0|1260759191|\n",
      "|     1|   2105|   4.0|1260759139|\n",
      "|     1|   2150|   3.0|1260759194|\n",
      "|     1|   2193|   2.0|1260759198|\n",
      "|     1|   2294|   2.0|1260759108|\n",
      "|     1|   2455|   2.5|1260759113|\n",
      "|     1|   2968|   1.0|1260759200|\n",
      "|     1|   3671|   3.0|1260759117|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "movies\n",
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|\n",
      "|     12|Dracula: Dead and...|       Comedy|Horror|\n",
      "|     13|        Balto (1995)|Adventure|Animati...|\n",
      "|     14|        Nixon (1995)|               Drama|\n",
      "|     15|Cutthroat Island ...|Action|Adventure|...|\n",
      "|     16|       Casino (1995)|         Crime|Drama|\n",
      "|     17|Sense and Sensibi...|       Drama|Romance|\n",
      "|     18|   Four Rooms (1995)|              Comedy|\n",
      "|     19|Ace Ventura: When...|              Comedy|\n",
      "|     20|  Money Train (1995)|Action|Comedy|Cri...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "movielens\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "|movieId|userId|rating| timestamp|               title|              genres|\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "|     31|     1|   2.5|1260759144|Dangerous Minds (...|               Drama|\n",
      "|   1029|     1|   3.0|1260759179|        Dumbo (1941)|Animation|Childre...|\n",
      "|   1061|     1|   3.0|1260759182|     Sleepers (1996)|            Thriller|\n",
      "|   1129|     1|   2.0|1260759185|Escape from New Y...|Action|Adventure|...|\n",
      "|   1172|     1|   4.0|1260759205|Cinema Paradiso (...|               Drama|\n",
      "|   1263|     1|   2.0|1260759151|Deer Hunter, The ...|           Drama|War|\n",
      "|   1287|     1|   2.0|1260759187|      Ben-Hur (1959)|Action|Adventure|...|\n",
      "|   1293|     1|   2.0|1260759148|       Gandhi (1982)|               Drama|\n",
      "|   1339|     1|   3.5|1260759125|Dracula (Bram Sto...|Fantasy|Horror|Ro...|\n",
      "|   1343|     1|   2.0|1260759131|    Cape Fear (1991)|            Thriller|\n",
      "|   1371|     1|   2.5|1260759135|Star Trek: The Mo...|    Adventure|Sci-Fi|\n",
      "|   1405|     1|   1.0|1260759203|Beavis and Butt-H...|Adventure|Animati...|\n",
      "|   1953|     1|   4.0|1260759191|French Connection...|Action|Crime|Thri...|\n",
      "|   2105|     1|   4.0|1260759139|         Tron (1982)|Action|Adventure|...|\n",
      "|   2150|     1|   3.0|1260759194|Gods Must Be Craz...|    Adventure|Comedy|\n",
      "|   2193|     1|   2.0|1260759198|       Willow (1988)|Action|Adventure|...|\n",
      "|   2294|     1|   2.0|1260759108|         Antz (1998)|Adventure|Animati...|\n",
      "|   2455|     1|   2.5|1260759113|     Fly, The (1986)|Drama|Horror|Sci-...|\n",
      "|   2968|     1|   1.0|1260759200| Time Bandits (1981)|Adventure|Comedy|...|\n",
      "|   3671|     1|   3.0|1260759117|Blazing Saddles (...|      Comedy|Western|\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "import numpy as np\n",
    "import math \n",
    "import time\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load data from the path to a dataframe called \"ratings\"\n",
    "## Small Dataset\n",
    "ratings = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://saltdean/data/movielens/ml-latest-small/ratings.csv\")\n",
    "## Large Dataset\n",
    "# ratings = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://saltdean/data/movielens/ml-20m/ratings.csv\")\n",
    "# Check which features are present \n",
    "print(\"ratings\")\n",
    "ratings.show()\n",
    "\n",
    "# Load data from path to dataframe called \"movies\"\n",
    "movies = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://saltdean/data/movielens/ml-latest-small/movies.csv\")\n",
    "# Check which features are present \n",
    "print(\"movies\")\n",
    "movies.show()\n",
    "\n",
    "# Join them into \"movielens\"\n",
    "print(\"movielens\")\n",
    "movielens = ratings.join(movies, \"movieId\")\n",
    "# Check which features are present \n",
    "movielens.show()\n",
    "\n",
    "# Cast data type from String to Integer and Double\n",
    "movielens = movielens.withColumn(\"movieId\", ratings[\"movieId\"].cast(IntegerType()))\n",
    "movielens = movielens.withColumn(\"rating\", ratings[\"rating\"].cast(DoubleType()))\n",
    "movielens = movielens.withColumn(\"timestamp\", ratings[\"timestamp\"].cast(IntegerType()))\n",
    "movielens = movielens.withColumn(\"userId\", ratings[\"userId\"].cast(IntegerType()))\n",
    "# ratings.take(3)\n",
    "\n",
    "# Print the types used in each column\n",
    "ratings = movielens\n",
    "ratings.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.2) Training and Testing the System\n",
    "- Split Training (80%) and Testing(20%) data\n",
    "- Change the training data size\n",
    "- Do a Grid Search to select the best model\n",
    "- Predict test data using the best model\n",
    "- Evaluate the best model's performance and time taken for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  850  rows\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training (80%) and hold-out testing data (20%)\n",
    "(traindata, test) = ratings.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "## Change the training data size\n",
    "# (training, garbage) = traindata.randomSplit([0.999, 0.001])   # ~80,000 rows\n",
    "# (training, garbage) = traindata.randomSplit([0.1, 0.9])     # ~8,000 rows\n",
    "(training, garbage) = traindata.randomSplit([0.01, 0.99])   # ~800 rows\n",
    "# (training, garbage) = traindata.randomSplit([0.001, 0.999]) # ~80 rows\n",
    "\n",
    "# Print training data size\n",
    "print('Training data size: ',training.count(),' rows') \n",
    "\n",
    "# Create an Alternate Least Square learning algorithm (Estimator)\n",
    "als = ALS(rank=10, \n",
    "          maxIter=5,\n",
    "          userCol=\"userId\",   \n",
    "          itemCol=\"movieId\",  \n",
    "          ratingCol=\"rating\")\n",
    "### numBlocks is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\n",
    "### rank is the number of latent factors in the model (defaults to 10).\n",
    "### maxIter is the maximum number of iterations to run (defaults to 10).\n",
    "### regParam specifies the regularization parameter in ALS (defaults to 1.0).\n",
    "\n",
    "## Create a ParamGridBuilder to construct a grid of parameters to search over. (ParameterMaps)\n",
    "paramGrid = ParamGridBuilder().addGrid(als.rank, [1,3,10,30,100])\\\n",
    "                                .addGrid(als.maxIter, [1,3,10,30,100])\\\n",
    "                                .build()\n",
    "            \n",
    "## No Grid Search\n",
    "# paramGrid = ParamGridBuilder().build() # * UnComment this line and comment the block above to run quickly\n",
    "\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# In this case the estimator is simply the linear regression. (Evaluator)\n",
    "regEval = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "####-------- TRAINING --------####\n",
    "# Get start time\n",
    "s=time.time()\n",
    "\n",
    "# Train and Validate models\n",
    "tvs = TrainValidationSplit(estimator=als,\n",
    "                            estimatorParamMaps=paramGrid,\n",
    "                            evaluator=regEval,\n",
    "                            # 80% of the data will be used for training, 20% for validation.\n",
    "                            trainRatio=0.8, seed=5)\n",
    "\n",
    "# Run TrainValidationSplit to train and choose the best set of parameters.\n",
    "model = tvs.fit(training)\n",
    "\n",
    "# Get the best model parameters\n",
    "# evaluatorMetrics = model.validationMetrics.copy()\n",
    "# minParams = np.argmin(evaluatorMetrics)\n",
    "# paramGrid[minParams]\n",
    "# print('paramGrid[minParams]:', paramGrid[minParams])\n",
    "best_model = model.bestModel\n",
    "maxIter = (best_model\n",
    "    ._java_obj     # Get Java object\n",
    "    .parent()      # Get parent (ALS estimator)\n",
    "    .getMaxIter()) # Get maxIter\n",
    "\n",
    "rank = best_model.rank\n",
    "print(\"rank:\", rank)\n",
    "print(\"maxIter:\", maxIter)\n",
    "print(\"regParam:\", maxIter) #### I CANT get this value (regParam) by ANY MEANS, so I took it off-I spent a lot of time trying...\n",
    "print('\\n-----------')\n",
    "\n",
    "# Get end time\n",
    "e=time.time()\n",
    "\n",
    "# Test the model's prediction in the hold-out Training data           \n",
    "predictions = model.transform(training)\n",
    "\n",
    "# Drop any rows with nan values from prediction (due to cold start problem)\n",
    "# predictions = predictions.dropna()\n",
    "predictions = predictions.fillna(0);\n",
    "\n",
    "# Evaluate the overall performance of the model by computing the Root-mean-square error (RMSE) on the Training data\n",
    "rmse = regEval.evaluate(predictions)\n",
    "print(\"Training Error (RMS) = \", round(rmse, 4))\n",
    "\n",
    "## Print the size of training data\n",
    "# print('Training data size: ',training.count(),' rows')  \n",
    "     \n",
    "# Print the time spent to train\n",
    "print('Training time: ',round(e-s, 4),' seconds')\n",
    "\n",
    "####-------- TESTING --------####\n",
    "# Get relative time\n",
    "s=time.time()\n",
    "\n",
    "# Test the model's prediction in the hold-out Test data           \n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Get end time\n",
    "e=time.time()\n",
    "\n",
    "# Drop or replace with 0 any rows with nan values from prediction (due to cold start problem)\n",
    "# predictions = predictions.dropna()\n",
    "predictions = predictions.fillna(0);\n",
    "               \n",
    "# Evaluate the overall performance of the model by computing the Root-mean-square error (RMSE) on the Test data\n",
    "rmse = regEval.evaluate(predictions)\n",
    "print('')\n",
    "print(\"Test Error (RMS) = \", round(rmse, 4))\n",
    "\n",
    "## Print the size of test data\n",
    "# print('Test data size: ',test.count(),' rows')  \n",
    "     \n",
    "# Print the time spent to test\n",
    "print('Test time: ',round(e-s, 4),' seconds')\n",
    "\n",
    "####------ Results for training and testing without Grid Search -------####\n",
    "## Training data size: ~80,000 rows\n",
    "# Training Error (RMS) =  0.6382  || Test Error (RMS) =  1.1182\n",
    "# Training time:  10.497  seconds || Test time:  0.0601  seconds\n",
    "\n",
    "## Training data size: ~8,000 rows\n",
    "# Training Error (RMS) =  0.2326  || Test Error (RMS) =  2.8208\n",
    "# Training time:  7.6864  seconds || Test time:  0.0603  seconds\n",
    "    \n",
    "## Training data size: ~800 rows\n",
    "# Training Error (RMS) =   0.0547 || Test Error (RMS) =  3.7398\n",
    "# Training time:  6.492  seconds  || Test time:  0.0514  seconds\n",
    " \n",
    "## Training data size: ~80 rows\n",
    "# Training Error (RMS) =  0.0501  || Test Error (RMS) =  3.7025\n",
    "# Training time:  5.419  seconds  || Test time:  0.0504  seconds\n",
    "\n",
    "# As we can see the larger the dataset the more computational expensive it is. \n",
    "# The training error is low and testing error is high for small dataset (meaning overfitting). \n",
    "# When the dataset is large, the model generalizes better, reducing overfitting.\n",
    "# One important point is that, only when the data set is really big (around 100,000) \n",
    "# is when the model starts to having a good prediction\n",
    "\n",
    "# The Parameter Grid Search was applied in the largest dataset size and the results are documented below:\n",
    "# Training Error (RMS) =  2.4116    || Test Error (RMS) =  3.2335\n",
    "# Training time:  251.6069  seconds || Test time:  0.0446  seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.3) Adding a new user and recommending movies to him"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_user_ID = 0\n",
    "\n",
    "# The format of each line is (userID, movieID, rating)\n",
    "new_user_ratings = [\n",
    "     (0,32,3),   # Twelve Monkeys\n",
    "     (0,589,5),  # Terminator 2\n",
    "     (0,50,4),   # Usual Suspects\n",
    "     (0,1080,4), # Monty Python \n",
    "     (0,1278,1), # Young Frankenstein\n",
    "     (0,1266,1), # Unforgiven \n",
    "     (0,1249,1), # Femme Nikita \n",
    "     (0,1090,1), # Platoon \n",
    "     (0,919,1) , # Wizard of Oz\n",
    "     (0,47,5)    # Seven \n",
    "    ]\n",
    "\n",
    "new_user_ratings_RDD = sc.parallelize(new_user_ratings)\n",
    "print('New user ratings: %s' % new_user_ratings_RDD.take(10))\n",
    "\n",
    "# Adding a personal new rating matrix \n",
    "\n",
    "df =[(0,32,3), # Twelve Monkeys\n",
    "     (0,589,5), # Terminator 2\n",
    "     (0,50,4), # Usual Suspects\n",
    "     (0,1080,4), # Monty Python \n",
    "     (0,1278,1), # Young Frankenstein\n",
    "     (0,1266,1), # Unforgiven \n",
    "     (0,1249,1), # Femme Nikita \n",
    "     (0,1090,1), # Platoon \n",
    "     (0,919,1) , # Wizard of Oz\n",
    "     (0,47,5)] # Seven \n",
    "\n",
    "df1 = sqlContext.createDataFrame(df)\n",
    "\n",
    "df1.collect()\n",
    "\n",
    "df1.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3) Conclusions and Discussions\n",
    "\n",
    "We implemented a collaborative filter to acess explicity the users-items ratings, and used the ALS algorithm which actually does not only do the Matrix Factorization but minimize the error for the predicted latented factors. Therefore, we achieve a system that can recommend movies based on the user's preferences (ratings). Although this is an efficient way to estimate non available ratings and therefore recommend products, there is still some issues which we could see through the work. First, the cold start problem which collaborative by itself cannot solve and . Second, it only really has a good accuracy with very large training data, otherwise it is not good. \n",
    "\n",
    "Implicit and explicity\n",
    "\n",
    "Reducing the training set size increases the root-mean square error\n",
    "\n",
    "\n",
    "#### References\n",
    "[1] Zhou, Y., Wilkinson, D., Schreiber, R. and Pan, R., 2008, June. Large-scale parallel collaborative filtering for the netflix prize. In International Conference on Algorithmic Applications in Management (pp. 337-348). Springer Berlin Heidelberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "https://sourceforge.net/p/jupiter/wiki/markdown_syntax/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
