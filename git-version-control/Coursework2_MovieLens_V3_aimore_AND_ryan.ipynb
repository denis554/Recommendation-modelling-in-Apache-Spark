{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# INM432 Big Data Coursework 2016/207 Part 2: Spark Pipelines and Evaluation of Scaling of Algorithms\n",
    "\n",
    "### Team Members: Ryan Nazareth and Aimore Dutra \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1) Introduction\n",
    "With the advent of the internet connecting everything and everyone, it became easy to one access a large amount of information. However, the facility to reach so much data also brought some problems. Consumers have to deal with an immeasurable number of items, loosing their time trying to find what they look for.\n",
    "\n",
    "Hence, big companies that have an immensity of products in their database are keen on advertising their products in a smart way helping their clients to find what they want.\n",
    "\n",
    "Nowadays, Recommendation Systems are being developed to address this problem.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.1) Task\n",
    "\n",
    "Our task is to create a Recommender System that can suggest new movies to users based on their preferences (ratings).\n",
    "\n",
    "There are several possible approaches for the recommendation task [1]:\n",
    "\n",
    "##### 1) Recommend the most popular items\n",
    "##### 2) Use a classifier to make recommendation\n",
    "##### 3) Collaborative Filtering\n",
    "\n",
    "#### We chose the Collaborative Filtering technique because this method gives more personalization and makes a more efficient use of data.\n",
    "\n",
    "The Collaborative Filtering approach has two main types:\n",
    "* a) User to User\n",
    "* b) Item to Item\n",
    "\n",
    "Item-item most of the time tends to be more accurate and computationally cheaper.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> #### References\n",
    "[1] https://www.analyticsvidhya.com/blog/2016/06/quick-guide-build-recommendation-engine-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.2) Dataset\n",
    "\n",
    "*Movies and most recently series have become a trend due to their current amazing quality and quantity at hand. Thanks to the advances in technology allowing them to be cheaper and quickly produced, there are millions of movies and series available.\n",
    "Not only more content is being created, but the existing ones are being stored. This has resulted in viewers having difficulties to find new video entertainment instances that they like.*\n",
    "\n",
    "\n",
    "\n",
    "The selected dataset for the coursework was the \"(ml-20m)\" from MovieLens, a movie recommendation service [1,2]. We made this choice because it has a lot of data and it most important it contains user ratings that allow us to use the Collaborative Filtering technique. The details of the dataset is below: \n",
    "\n",
    "- 27,278 movies (with 19 different Genres)\n",
    "- 138,493 users\n",
    "- 465,564 tag applications \n",
    "- and 20,000,263 ratings (from 1-5 stars)\n",
    "\n",
    "These data were created by  users between January 09, 1995 and March 31, 2015.\n",
    "\n",
    "The data are divided in six files, containing each:\n",
    "- genome-scores.csv: MovieID::TagId::relevance\n",
    "- genome-tags.csv:   TagId::Tag\n",
    "- links.csv:         MovieID::imdbID::tmdbID\n",
    "- movies.csv:        MovieID::Title::Genres\n",
    "- ratings.csv:       UserID::MovieID::Rating::Timestamp\n",
    "- tags.csv:          UserID::MovieID::Tag::Timestamp\n",
    "\n",
    "\n",
    "> #### References\n",
    "[1] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872\n",
    "\n",
    ">[2] http://files.grouplens.org/datasets/movielens/ml-20m-README.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.3) Technique Used\n",
    "Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix. spark.ml currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. spark.ml uses the alternating least squares (ALS) algorithm to learn these latent factors. The implementation in spark.ml has the following parameters:\n",
    "\n",
    "- numBlocks is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\n",
    "- rank is the number of latent factors in the model (defaults to 10).\n",
    "- maxIter is the maximum number of iterations to run (defaults to 10).\n",
    "- regParam specifies the regularization parameter in ALS (defaults to 1.0).\n",
    "- implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false which means using explicit feedback).\n",
    "- alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0).\n",
    "- nonnegative specifies whether or not to use nonnegative constraints for least squares (defaults to false).\n",
    "\n",
    "> #### References\n",
    "[1] https://spark.apache.org/docs/latest/ml-collaborative-filtering.html\n",
    "\n",
    ">[2] https://spark.apache.org/docs/latest/ml-tuning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "# 2) Code\n",
    "\n",
    "\n",
    "### Loading data and applying transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "import math \n",
    "import time\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load data from the path to a dataframe called \"ratings\"\n",
    "## Small Dataset\n",
    "ratings = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://saltdean/data/movielens/ml-latest-small/ratings.csv\")\n",
    "## Large Dataset\n",
    "# ratings = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://saltdean/data/movielens/ml-20m/ratings.csv\")\n",
    "# lets check which features are present \n",
    "ratings.describe() \n",
    "\n",
    "# Cast data type from String to Integer and Double\n",
    "ratings = ratings.withColumn(\"movieId\", ratings[\"movieId\"].cast(IntegerType()))\n",
    "ratings = ratings.withColumn(\"rating\", ratings[\"rating\"].cast(DoubleType()))\n",
    "ratings = ratings.withColumn(\"timestamp\", ratings[\"timestamp\"].cast(IntegerType()))\n",
    "ratings = ratings.withColumn(\"userId\", ratings[\"userId\"].cast(IntegerType()))\n",
    "# ratings.take(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.1) Approach 1:\n",
    "- Split Training (80%) and Testing(20%) data\n",
    "- Do a Grid Search to select the best model\n",
    "- Predict test data using the best model\n",
    "- Evaluate the best model's performance and time taken for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  79926  rows\n",
      "3\n",
      "4\n",
      "The best model was trained with rank 3\n",
      "\n",
      "\n",
      "Training Error (RMS) =  2.9305\n",
      "Training time:  238.3416  seconds\n",
      "\n",
      "Test Error (RMS) =  3.6199\n",
      "Test time:  0.0432  seconds\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training (80%) and hold-out testing data (20%)\n",
    "(traindata, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "## Change the amount of training data\n",
    "(training, garbage) = traindata.randomSplit([0.999, 0.001]) # ~80,000 rows\n",
    "# (training, garbage) = traindata.randomSplit([0.1, 0.9]) # ~8,000 rows\n",
    "# (training, garbage) = traindata.randomSplit([0.01, 0.99]) # ~800 rows\n",
    "# (training, garbage) = traindata.randomSplit([0.001, 0.999]) # ~80 rows\n",
    "\n",
    "# Print training data size\n",
    "print('Training data size: ',training.count(),' rows') \n",
    "\n",
    "# Create an Alternate Least Square learning algorithm\n",
    "als = ALS(regParam=0.1, rank=10, maxIter=3, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
    "\n",
    "## Create a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "paramGrid = ParamGridBuilder().addGrid(als.regParam, [0.03,0.1,0.3])\\\n",
    "                                .addGrid(als.rank, [3,10,30])\\\n",
    "                                .addGrid(als.maxIter, [1,3,10])\\\n",
    "                                .build()\n",
    "\n",
    "## Grid Search Rank\n",
    "# paramGrid = ParamGridBuilder().addGrid(als.rank, [1,3,10,30,100]).build()\n",
    "\n",
    "## No Grid Search\n",
    "# paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "print('3')\n",
    "\n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "regEval = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "####-------- TRAIN --------####\n",
    "# Get start time\n",
    "s=time.time()\n",
    "\n",
    "print('4')\n",
    "\n",
    "# Train and Validate models\n",
    "tvs = TrainValidationSplit(estimator=als,\n",
    "                            estimatorParamMaps=paramGrid,\n",
    "                            evaluator=regEval,\n",
    "                            # 80% of the data will be used for training, 20% for validation.\n",
    "                            trainRatio=0.8, seed=5)\n",
    "\n",
    "## Run TrainValidationSplit to choose the best set of parameters.\n",
    "model = tvs.fit(training)\n",
    "# print ('The best model was trained with rank %d'%model.bestModel.regParam)\n",
    "# print ('The best model was trained with rank %d'%model.bestModel.maxIter)\n",
    "print ('The best model was trained with rank %d'%model.bestModel.rank)\n",
    "print ('The best model was trained with rank %d'%model.bestModel)\n",
    "print('')\n",
    "\n",
    "# Get end time\n",
    "e=time.time()\n",
    "\n",
    "# Test the model's prediction in the hold-out Training data           \n",
    "predictions = model.transform(training)\n",
    "\n",
    "# Drop any rows with nan values from prediction (due to cold start problem)\n",
    "# predictions = predictions.dropna()\n",
    "predictions = predictions.fillna(0);\n",
    "\n",
    "# Evaluate the overall performance of the model by computing the Root-mean-square error (RMSE) on the Training data\n",
    "rmse = regEval.evaluate(predictions)\n",
    "print('')\n",
    "print(\"Training Error (RMS) = \", round(rmse, 4))\n",
    "\n",
    "# Print the size of training data\n",
    "# print('Training data size: ',training.count(),' rows')  \n",
    "     \n",
    "# Print the time spent to train\n",
    "print('Training time: ',round(e-s, 4),' seconds')\n",
    "\n",
    "####-------- TEST --------####\n",
    "# Get relative time\n",
    "s=time.time()\n",
    "\n",
    "# Test the model's prediction in the hold-out Test data           \n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Get end time\n",
    "e=time.time()\n",
    "\n",
    "# Drop or replace with 0 any rows with nan values from prediction (due to cold start problem)\n",
    "# predictions = predictions.dropna()\n",
    "predictions = predictions.fillna(0);\n",
    "               \n",
    "# Evaluate the overall performance of the model by computing the Root-mean-square error (RMSE) on the Test data\n",
    "rmse = regEval.evaluate(predictions)\n",
    "print('')\n",
    "print(\"Test Error (RMS) = \", round(rmse, 4))\n",
    "\n",
    "# Print the size of test data\n",
    "# print('Test data size: ',test.count(),' rows')  \n",
    "     \n",
    "# Print the time spent to test\n",
    "print('Test time: ',round(e-s, 4),' seconds')\n",
    "\n",
    "#### Conclusions regarding the training and testing time without Grid Search ####\n",
    "## Data size: ~80,000 rows\n",
    "# Training Error (RMS) =  0.6382  || Test Error (RMS) =  1.1182\n",
    "# Training time:  10.497  seconds || Test time:  0.0601  seconds\n",
    "\n",
    "## Data size: ~8,000 rows\n",
    "# Training Error (RMS) =  0.2326  || Test Error (RMS) =  2.8208\n",
    "# Training time:  7.6864  seconds  || Test time:  0.0603  seconds\n",
    "    \n",
    "## Training data size: ~800 rows\n",
    "# Training Error (RMS) =   0.0547  || Test Error (RMS) =  3.7398\n",
    "# Training time:  6.492  seconds  || Test time:  0.0514  seconds\n",
    " \n",
    "## Training data size: ~80 rows\n",
    "# Training Error (RMS) =  0.0501  || Test Error (RMS) =  3.7025\n",
    "# Training time:  5.419  seconds  || Test time:  0.0504  seconds\n",
    "\n",
    "# As we can see the larger the dataset the more computational expensive it is. \n",
    "# The training error is low and testing error is high for small dataset (meaning overfitting). \n",
    "# When the dataset is large, the model generalizes better, reducing overfitting.\n",
    "# One important point is that, only when the data set is really big (around 100,000) \n",
    "# is when the model starts to having a good prediction\n",
    "\n",
    "# The Parameter Grid Search was applied in the largest dataset size and the results are documented below:\n",
    "# Training Error (RMS) =  2.4116    || Test Error (RMS) =  3.2335\n",
    "# Training time:  251.6069  seconds || Test time:  0.0446  seconds\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3) Conclusions and Discussions\n",
    "\n",
    "We implemented a collaborative filter to acess explicity the users-items ratings, and used the ALS algorithm which actually does not only do the Matrix Factorization but minimize the error for the predicted latented factors. Therefore, we achieve a system that can recommend movies based on the user's preferences (ratings). However, this is an efficient way, there is still the cold start problem which collaborative only cannot solve.\n",
    "\n",
    "Reducing the training set size increases the root-mean square error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "https://sourceforge.net/p/jupiter/wiki/markdown_syntax/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
