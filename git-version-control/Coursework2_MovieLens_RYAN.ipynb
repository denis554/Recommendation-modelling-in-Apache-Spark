{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# INM432 Big Data Coursework 2016/207 Part 2: Spark Pipelines and Evaluation of Scaling of Algorithms\n",
    "\n",
    "### Team Members: Ryan Nazareth and Aimore Dutra "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## a) Choice of Dataset and Task \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[summary: string, movieId: string, rating: string, timestamp: string, userId: string]\n",
      "1210\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "# the imports are used creating the data frame\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate() # create a SparkSession \n",
    "# this gets us an RDD. (could also be done with RDD.textFile in this case)\n",
    "lines = spark.read.text(\"hdfs://saltdean/data/movielens/sample_movielens_ratings.txt\").rdd \n",
    "# now split the lines at the '::'\n",
    "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "ratings.createOrReplaceTempView('ratings') # register the DataFrame so that we can use it with Spark SQL.\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2]) # split into test and training set\n",
    "print(training.describe()) # just for testing, should show the four columns\n",
    "print(training.count()) # just fore testing, should be around 1188"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## b) Machine Learning Pipeline \n",
    "\n",
    "Now create an ALS estimator and a parameter grid to explore different values for the `rank` and `regParam` parameter of the ALS. \n",
    "\n",
    "### Choice of processing steps:\n",
    "\n",
    "### Learning algorithms: \n",
    "\n",
    "### Parameter settings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanRating 1.7741505662891406\n",
      "sedf DataFrame[se: double]\n",
      "meanSE 1.662126327662754\n"
     ]
    }
   ],
   "source": [
    "SQL1 = 'SELECT AVG(rating) FROM ratings'\n",
    "row = spark.sql(SQL1).collect()[0] # get the single row with the result\n",
    "\n",
    "meanRating = row['avg(rating)'] # access Row as a map \n",
    "print('meanRating',meanRating)\n",
    "\n",
    "se_rdd = test.rdd.map(lambda row: Row(se = pow(row['rating']-meanRating,2)) ) \n",
    "se_df = spark.createDataFrame(se_rdd) \n",
    "se_df.createOrReplaceTempView('se')\n",
    "print('sedf',se_df)\n",
    "SQL2 = 'SELECT AVG(se) FROM se'\n",
    "row = spark.sql(SQL2).collect()[0]\n",
    "meanSE = row['avg(se)'] # access Row as a map \n",
    "print('meanSE',meanSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## c) Evaluating Performance of Pipeline using training and test sets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## d) Implementing a parameter grid \n",
    "\n",
    "Implementing a parameter grid (using pyspark.ml.tuning.ParamGridBuilder[source]), varying at least one feature preprocessing step, one machine learning parameter, and the training set size. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting cross-validation\n",
      "finished cross-validation\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit \n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, rank=5, regParam=0.1, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
    "regEval = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(als.regParam, [0.03,0.1,0.3]) \\\n",
    "    .addGrid(als.rank, [5,10,50]).build()\n",
    "    #    .addGrid(als.rank, [3,10,30,100,300]).build() \n",
    "\n",
    "tvs = TrainValidationSplit(estimator=als, estimatorParamMaps=paramGrid, evaluator=regEval)\n",
    "print('starting training')\n",
    "tvsModel = tvs.fit(training)\n",
    "print('finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "Row(movieId=31, rating=3.0, timestamp=1424380312, userId=8, prediction=1.852697730064392)\n",
      "Row(movieId=31, rating=2.0, timestamp=1424380312, userId=25, prediction=1.506085753440857)\n",
      "Row(movieId=31, rating=1.0, timestamp=1424380312, userId=24, prediction=1.7374300956726074)\n",
      "Row(movieId=31, rating=1.0, timestamp=1424380312, userId=0, prediction=1.0014280080795288)\n",
      "Row(movieId=85, rating=5.0, timestamp=1424380312, userId=16, prediction=1.7288849353790283)\n",
      "Root-mean-square error = 1.2269074565300253\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tvsModel.bestModel.rank)\n",
    "#paramMap = list(zip(tvsModel.getEstimatorParamMaps())\n",
    "#paramMax = max(paramMap)\n",
    "#print(paramMax)\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = tvsModel.transform(test)\n",
    "for row in predictions.take(5):\n",
    "    print(row)\n",
    "    \n",
    "rmse = regEval.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
