{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lab Sheet 8: Using ALS with Spark, evaluating CV results \n",
    "\n",
    "These tasks are for working in the lab session and during the week. We will use the Spark ALS and explore the effects of outcome of a cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task 1)\n",
    "Read the data, split into tokens and create a structured DataFrame. For low level tasks like splitting strings, we need to use an RDD, where we can apply a `map` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[summary: string, movieId: string, rating: string, timestamp: string, userId: string]\n",
      "1183\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "# the imports are used creating the data frame\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate() # create a SparkSession \n",
    "# this gets us an RDD. (could also be done with RDD.textFile in this case)\n",
    "lines = spark.read.text(\"hdfs://saltdean/data/movielens/sample_movielens_ratings.txt\").rdd \n",
    "# now split the lines at the '::'\n",
    "parts = lines.map( ... ) # <<<\n",
    "ratingsRDD = parts.map( ... ) # <<< create a Row userId as int, movieId  as int, rating as float, timestamp as int\n",
    "ratings = spark.createDataFrame( ... ) # create a dataframe\n",
    "ratings.createOrReplaceTempView('ratings') # register the DataFrame so that we can use it with Spark SQL.\n",
    "(training, test) =  # create a random split into test and training set from the dataframe\n",
    "print(training.describe()) # just for testing, should show the four columns\n",
    "print(training.count()) # just fore testing, should be around 1188"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Task 2)\n",
    "\n",
    "Now take a very simple estimate as the baseline: calculate the mean of all ratings.    \n",
    "\n",
    "The average can be calculated with the SQL `AVG` command, within an SQL `SELECT` statement. If you replace selected column, e.g. `rating`, with `AVG(rating)`, the returned DataFrame will contain only 1 row. You can access the contents of the rows by its name, e.g. `row['avg(rating)']` (the avg needs to be lower case here). \n",
    "\n",
    "Then calculate the squared error with respect to the average (as predictor). You can again use the SQL `AVG` command. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanRating 1.7741505662891406\n",
      "se_df DataFrame[se: double]\n",
      "meanSE 1.5408512158621657\n"
     ]
    }
   ],
   "source": [
    "# select the average from the ratings\n",
    "SQL1 = ' ... '\n",
    "row = spark.sql(SQL1).collect()[0] # get the single row with the result\n",
    "print('row', row)\n",
    "meanRating = ... # access Row as a map \n",
    "print('meanRating', meanRating)\n",
    "\n",
    "se_rdd = test.rdd.map( ... ) #<<< get the the squared error (difference to the average) using Python pow() \n",
    "se_df = spark.createDataFrame(se_rdd) # create a data framd\n",
    "se_df.createOrReplaceTempView( ... ) #<<< Register with the SQL system (choose a name)\n",
    "print('se_df',se_df) \n",
    "\n",
    "# get the average squared error\n",
    "SQL2 = '...'\n",
    "row = spark.sql(SQL2).collect()[0]\n",
    "meanSE = ... #<<< access Row as a map \n",
    "print('meanSE',meanSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task 3: \n",
    "\n",
    "Now create an ALS estimator and a parameter grid to explore different values for the `rank` and `regParam` parameter of the ALS. Then build a cross-validator to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting cross-validation\n",
      "finished cross-validation\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, rank=5, regParam=0.1, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
    "\n",
    "# build a paramter grid for rank and regParam\n",
    "paramGrid = ParamGridBuilder() # <<<<\n",
    "\n",
    "# set up a regression evaluater evaluating RMSE \n",
    "regEval = RegressionEvaluator( ... ) # <<<<\n",
    "\n",
    "# set up a cross validator with the als, paramGrid and regEval\n",
    "crossVal = CrossValidator( ... numFolds=3) # <<<<\n",
    "\n",
    "print('starting cross-validation')\n",
    "cvModel = crossVal.fit(training)\n",
    "print('finished cross-validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task 4)\n",
    "\n",
    "Take the trained cvModel and extract the best parameter values by inspecting the estimatorParameterMap. Compare the RMSE value to that of the mean for different parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({Param(parent='ALS_41f7b157fba36f60120c', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent='ALS_41f7b157fba36f60120c', name='rank', doc='rank of the factorization'): 10}, nan)]\n",
      "({Param(parent='ALS_41f7b157fba36f60120c', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent='ALS_41f7b157fba36f60120c', name='rank', doc='rank of the factorization'): 10}, nan)\n",
      "Root-mean-square error = 1.0302617871748263\n"
     ]
    }
   ],
   "source": [
    "print(cvModel.avgMetrics) # the metrics form the CrossValidation\n",
    "print(cvModel.getEstimatorParamMaps()) # gives you the parameter combinations, print it out, too\n",
    "# use Python zip and list to create a joint paramter and result map\n",
    "paramMap = ... \n",
    "print(paramMap)\n",
    "# use Python max to get the best params \n",
    "paramMax = max(paramMap, key=lambda x: x[1])\n",
    "print(paramMax)\n",
    "\n",
    "# Evaluate the cvModel by computing the RMSE on the test data\n",
    "predictions = ... #<<<\n",
    "rmse = regEval.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task 5) \n",
    "\n",
    "Apply the apporach above to the larger MovieLens dataset (or part of it). The data is available at `/data/tempstore/movielens/ml-latest-small` "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
